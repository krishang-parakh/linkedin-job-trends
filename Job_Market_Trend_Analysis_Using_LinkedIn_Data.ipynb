{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishang-parakh/linkedin-job-trends/blob/main/Job_Market_Trend_Analysis_Using_LinkedIn_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZAd1fq_2Cia",
        "outputId": "663529ae-5fee-40d2-feb9-4bf897b0dc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.11/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from graphframes) (2.0.2)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.11/dist-packages (from graphframes) (1.3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install graphframes\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"LinkedIn Job Postings\").master(\"local[*]\").config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.1-spark3.0-s_2.12\").getOrCreate()\n",
        "from pyspark.ml.feature import Imputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = {\n",
        "    \"benefits\": \"benefits.csv\",\n",
        "    \"companies\": \"companies.csv\",\n",
        "    \"company_industries\": \"company_industries.csv\",\n",
        "    \"company_specialities\": \"company_specialities.csv\",\n",
        "    \"employee_counts\": \"employee_counts.csv\",\n",
        "    \"industries\": \"industries.csv\",\n",
        "    \"job_industries\": \"job_industries.csv\",\n",
        "    \"job_postings\": \"job_postings.csv\",\n",
        "    \"job_skills\": \"job_skills.csv\",\n",
        "    \"salaries\": \"salaries.csv\",\n",
        "    \"skills\": \"skills.csv\",\n",
        "}"
      ],
      "metadata": {
        "id": "H7In8tNu25dj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming simplified schema for demonstration, adjust according to your CSV files\n",
        "job_postings_schema = \"job_id STRING, title STRING, description STRING, location STRING, company_id STRING\"\n",
        "job_industries_schema = \"job_id STRING, industry_id STRING\"\n",
        "job_skills_schema = \"job_id STRING, skill_abr STRING\"\n",
        "industries_schema = \"industry_id STRING, industry_name STRING\"\n",
        "skills_schema = \"skill_abr STRING, skill_name STRING\"\n",
        "salaries_schema = \"job_id STRING, salary INT, currency STRING\"\n",
        "employee_counts_schema = \"company_id STRING, employee_count INT\"\n",
        "\n",
        "# Read CSV files into DataFrames, assuming header=True and inferring schema for demonstration\n",
        "job_postings_df = spark.read.option(\"header\", \"true\").csv(\"job_postings.csv\", inferSchema=True)\n",
        "job_industries_df = spark.read.option(\"header\", \"true\").csv(\"job_industries.csv\", inferSchema=True)\n",
        "job_skills_df = spark.read.option(\"header\", \"true\").csv(\"job_skills.csv\", inferSchema=True)\n",
        "industries_df = spark.read.option(\"header\", \"true\").csv(\"industries.csv\", inferSchema=True)\n",
        "skills_df = spark.read.option(\"header\", \"true\").csv(\"skills.csv\", inferSchema=True)\n",
        "\n",
        "# Perform joins\n",
        "joined_df = job_postings_df \\\n",
        "    .join(job_industries_df, \"job_id\") \\\n",
        "    .join(industries_df, \"industry_id\") \\\n",
        "    .join(job_skills_df, \"job_id\") \\\n",
        "    .join(skills_df, \"skill_abr\")"
      ],
      "metadata": {
        "id": "6_NEROKQY8yy",
        "outputId": "1733618a-9157-46ee-f9bc-7d073c80cd3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/job_postings.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-1913764723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Read CSV files into DataFrames, assuming header=True and inferring schema for demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mjob_postings_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_postings.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mjob_industries_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_industries.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mjob_skills_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_skills.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/job_postings.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import time\n",
        "\n",
        "streaming_directory = \"stream_simulation/\"\n",
        "streaming_checkpoint_directory = \"stream_checkpoint/\"\n",
        "\n",
        "# Convert the DataFrame to JSON and save it to simulate the initial load\n",
        "joined_df.write.mode(\"overwrite\").json(streaming_directory)\n",
        "\n",
        "# Assuming the schema of joined_df is correctly defined and does not include duplicate columns\n",
        "stream_df = spark.readStream.schema(joined_df.schema).json(streaming_directory)\n",
        "\n",
        "# Start the streaming query to output to a memory table\n",
        "query = stream_df.writeStream.queryName(\"streaming_data\").format(\"memory\").outputMode(\"update\").start()\n",
        "\n",
        "# Function to perform SQL queries and display results\n",
        "def display_streaming_data():\n",
        "    # Display top industries by job postings\n",
        "    print(\"Job Postings by Industry:\\n\")\n",
        "    spark.sql(\"\"\"\n",
        "        SELECT industry_name, COUNT(*) AS job_count\n",
        "        FROM streaming_data\n",
        "        GROUP BY industry_name\n",
        "        ORDER BY job_count DESC\n",
        "    \"\"\").show(truncate=False)\n",
        "\n",
        "    # Display top skills by job postings\n",
        "    print(\"Job Availability by Skill Type:\\n\")\n",
        "    spark.sql(\"\"\"\n",
        "        SELECT skill_name, COUNT(*) AS job_count\n",
        "        FROM streaming_data\n",
        "        GROUP BY skill_name\n",
        "        ORDER BY job_count DESC\n",
        "    \"\"\").show(truncate=False)\n",
        "\n",
        "    # Select skill names along with the list of industries that require them\n",
        "    print(\"List of Industries by Skills Required:\\n\")\n",
        "    spark.sql(\"\"\"\n",
        "        SELECT job_skills.skill_name, collect_list(distinct job_industries.industry_name) FROM job_skills JOIN job_industries\n",
        "        ON job_skills.job_id = job_industries.job_id\n",
        "        GROUP BY\n",
        "          job_skills.skill_name\n",
        "    \"\"\").show(truncate=False)\n",
        "\n",
        "# Simulate streaming by appending new data\n",
        "for i in range(1, 16):\n",
        "    print(f\"Iteration: {i}\")\n",
        "    # Specify the source file for the initial data\n",
        "    source_file = streaming_directory + \"part-00000\"  # This assumes only one part file exists; adjust as necessary\n",
        "    destination_file = streaming_directory + f\"data_{i}.json\"\n",
        "\n",
        "    time.sleep(5)  # Adjust based on your system\n",
        "    display_streaming_data()\n",
        "\n",
        "# Stop the query\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "jGqvl2JLZcdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, path in file_paths.items():\n",
        "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
        "    rdd = df.rdd\n",
        "    columns = df.columns  # Extract column names\n",
        "    count = rdd.count()  # Get the number of observations in the RDD\n",
        "\n",
        "    # Print dataset name, number of observations, and columns\n",
        "    print(f\"Dataset Name: {name}\")\n",
        "    print(f\"Number of Observations: {count}\")\n",
        "    print(\"Columns:\", columns)\n",
        "\n",
        "    # For each column, print the schema description (data type)\n",
        "    # Since RDDs do not have schema, we extract schema from DataFrame\n",
        "    for field in df.schema.fields:\n",
        "        print(f\"{field.name}: {field.dataType.simpleString()}\")\n",
        "\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "Q3iIjBBPZigW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Assuming job_postings_df is already defined as per your schema\n",
        "job_postings_df = spark.read.csv(\"job_postings.csv\", header=True, schema=job_postings_schema)\n",
        "\n",
        "# Utilize DataFrame API to count nulls directly without converting to RDD\n",
        "null_counts = job_postings_df.select([count(when(col(c).isNull(), c)).alias(c) for c in job_postings_df.columns])\n",
        "\n",
        "print(\"Count of Null Values in Each Field:\")\n",
        "null_counts.show()"
      ],
      "metadata": {
        "id": "gJRoUT-SV6ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDQlJW9m3oKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you're back to static DataFrames for RDD operations\n",
        "# Let's take job_postings_df as an example\n",
        "job_postings_df = spark.read.csv(\"job_postings.csv\", schema=job_postings_schema, header=True)\n",
        "\n",
        "# Convert to RDD for manipulation\n",
        "job_postings_rdd = job_postings_df.rdd\n",
        "\n",
        "# Column descriptions\n",
        "print(\"Column Descriptions:\")\n",
        "print(job_postings_df.dtypes)\n",
        "\n",
        "# Initial few rows\n",
        "print(\"Initial Rows:\")\n",
        "for row in job_postings_rdd.take(5):\n",
        "    print(row)\n",
        "\n",
        "print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "id": "pYanSlpnTouW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_conditions = \"\"\"\n",
        "For companies in the same city:\n",
        "- \"Rivals\": Both have a size >= 5.\n",
        "- \"Partners\": One has a size >= 5 and the other has a size between 3 and 5.\n",
        "- \"Unrelated\": One has a size >= 5 and the other has a size < 3.\n",
        "- \"Mergers\": Both have a size < 5 but >= 3.\n",
        "- \"Allies\": One has a size >= 3 but < 5, and the other has a size < 3.\n",
        "- \"Sisters\": Both have a size < 3.\n",
        "\n",
        "For companies in different cities:\n",
        "- \"Allies\": If only one has a size >= 4.\n",
        "- \"Partners\": If both have a size >= 4.\n",
        "- \"Sisters\": If both have a size <= 3.\n",
        "- \"Rivals\": For all other conditions.\n",
        "\"\"\"\n",
        "\n",
        "# Output the structured conditions\n",
        "print(structured_conditions)"
      ],
      "metadata": {
        "id": "CeMa2RuExEeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df = spark.read.csv(\"Nodes.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Self-join the DataFrame to create all possible pairs of companies\n",
        "edges_df = nodes_df.alias(\"src\").join(nodes_df.alias(\"dst\"), col(\"src.city\") == col(\"dst.city\"), \"inner\") \\\n",
        "    .select(\n",
        "        col(\"src.id\").alias(\"src\"),\n",
        "        col(\"dst.id\").alias(\"dst\"),\n",
        "        when(\n",
        "            (col(\"src.size\") >= 5) & (col(\"dst.size\") >= 5), \"rivals\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") >= 5) & (col(\"dst.size\") < 5) & (col(\"dst.size\") >= 3), \"partners\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") >= 5) & (col(\"dst.size\") < 3), \"unrelated\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") < 5) & (col(\"src.size\") >= 3) & (col(\"dst.size\") < 5) & (col(\"dst.size\") >= 3), \"mergers\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") >= 3) & (col(\"src.size\") < 5) & (col(\"dst.size\") < 3), \"allies\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") < 3) & (col(\"dst.size\") < 3), \"sisters\"\n",
        "        ).otherwise(\"rivals\").alias(\"relation\")\n",
        "    )\n",
        "\n",
        "# Filter out duplicate pairs and self-relationships\n",
        "edges_df = edges_df.filter(\"src != dst\").distinct()\n",
        "\n",
        "# Display the edges DataFrame\n",
        "edges_df.show(truncate=False)\n",
        "\n",
        "# For different cities, we have to create additional rules and append to the current edges DataFrame\n",
        "different_city_edges_df = nodes_df.alias(\"src\").join(nodes_df.alias(\"dst\"), col(\"src.city\") != col(\"dst.city\"), \"inner\") \\\n",
        "    .select(\n",
        "        col(\"src.id\").alias(\"src\"),\n",
        "        col(\"dst.id\").alias(\"dst\"),\n",
        "        when(\n",
        "            (col(\"src.size\") >= 4) & (col(\"dst.size\") >= 4), \"partners\"\n",
        "        ).when(\n",
        "            (col(\"src.size\") <= 3) & (col(\"dst.size\") <= 3), \"sisters\"\n",
        "        ).otherwise(\"rivals\").alias(\"relation\")\n",
        "    )\n",
        "\n",
        "# Filter out duplicate pairs and self-relationships\n",
        "different_city_edges_df = different_city_edges_df.filter(\"src != dst\").distinct()\n",
        "\n",
        "# Union the two datasets\n",
        "final_edges_df = edges_df.union(different_city_edges_df).distinct()\n",
        "\n",
        "# Write the edges DataFrame to a CSV file\n",
        "final_edges_df.write.csv(\"Edges.csv\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "Rtg7UBrY33fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from graphframes import GraphFrame\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vertices = spark.read.option('header', 'true').csv('Nodes.csv')\n",
        "edges = spark.read.option('header', 'true').csv('Edges.csv')\n",
        "\n",
        "vertices.show(5)\n",
        "edges.show(5)\n",
        "\n",
        "mygraph = GraphFrame(vertices, edges)\n",
        "\n",
        "mygraph.vertices.show()\n",
        "mygraph.edges.show()\n",
        "\n",
        "# result = mygraph.filterEdges(\"relationship = 'requires_skill'\").filterVertices(\"size = 7\")\n",
        "# result.vertices.show()\n",
        "# result.edges.show()\n",
        "\n",
        "def plot_undirected_graph(edge_list):\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    gplot = nx.Graph()\n",
        "    for row in edge_list.select(\"src\", \"dst\").take(100):\n",
        "        gplot.add_edge(row[\"src\"], row[\"dst\"])\n",
        "    nx.draw(gplot, with_labels=True, font_weight=\"bold\", node_size=500)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JhySZs2R34xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_undirected_graph(mygraph.edges)"
      ],
      "metadata": {
        "id": "8UXIVlhJESbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "job_postings_df = spark.read.csv(\"job_postings.csv\", header=True, inferSchema=True)\n",
        "salaries_df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
        "\n",
        "data_df = job_postings_df.join(salaries_df, \"job_id\")"
      ],
      "metadata": {
        "id": "MB2XtwK32MlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = job_postings_df.columns  # Replace with actual feature columns\n",
        "feature_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Prepare the data with features and label\n",
        "data_df = feature_assembler.transform(data_df).select(col(\"salary\").alias(\"label\"), \"features\")\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = data_df.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "1YkzjhN32ytp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "lr = LinearRegression()\n",
        "dt = DecisionTreeRegressor()\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Train the models\n",
        "lr_model = lr.fit(train_df)\n",
        "dt_model = dt.fit(train_df)\n",
        "rf_model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "lr_predictions = lr_model.transform(test_df)\n",
        "dt_predictions = dt_model.transform(test_df)\n",
        "rf_predictions = rf_model.transform(test_df)\n",
        "\n",
        "# Evaluate the models\n",
        "evaluator = RegressionEvaluator()\n",
        "\n",
        "lr_rmse = evaluator.evaluate(lr_predictions)\n",
        "dt_rmse = evaluator.evaluate(dt_predictions)\n",
        "rf_rmse = evaluator.evaluate(rf_predictions)\n",
        "\n",
        "# Print RMSE values\n",
        "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
        "print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
        "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
        "\n",
        "# Determine the best model based on RMSE\n",
        "best_model = min([(lr_model, lr_rmse), (dt_model, dt_rmse), (rf_model, rf_rmse)], key=lambda x: x[1])\n",
        "print(f\"The best model is: {best_model[0].__class__.__name__} with a RMSE of: {best_model[1]}\")"
      ],
      "metadata": {
        "id": "aMVFfSuP26W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WzM9B7juXqDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}